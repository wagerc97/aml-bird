{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bird Species Classifier for AML project using TensorFlow\n## University of Vienna, SS 2022","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:35:47.603306Z","iopub.execute_input":"2022-05-27T16:35:47.603691Z","iopub.status.idle":"2022-05-27T16:35:47.607886Z","shell.execute_reply.started":"2022-05-27T16:35:47.603659Z","shell.execute_reply":"2022-05-27T16:35:47.607014Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras import layers, Model\n\n# To search directories\nimport os\nimport glob\n\n# To visualize data\nimport PIL\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#sns.set_style('darkgrid')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:04.191160Z","iopub.execute_input":"2022-05-29T22:25:04.191824Z","iopub.status.idle":"2022-05-29T22:25:04.197293Z","shell.execute_reply.started":"2022-05-29T22:25:04.191780Z","shell.execute_reply":"2022-05-29T22:25:04.196497Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# 1. Examine and understand data\n## CSV data\nThe ``birds.csv`` contains information of the dataset. Let's look into the structure of the data.","metadata":{}},{"cell_type":"code","source":"# Create a dataframe from the csv\nbirds_df = pd.read_csv(\"../input/100-bird-species/birds.csv\")\n# clean column names\nbirds_df.columns = [col.replace(' ', '_').lower() for col in birds_df.columns]\nbirds_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:04.220071Z","iopub.execute_input":"2022-05-29T22:25:04.220430Z","iopub.status.idle":"2022-05-29T22:25:04.298941Z","shell.execute_reply.started":"2022-05-29T22:25:04.220403Z","shell.execute_reply":"2022-05-29T22:25:04.298027Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"birds_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:04.300773Z","iopub.execute_input":"2022-05-29T22:25:04.301196Z","iopub.status.idle":"2022-05-29T22:25:04.333413Z","shell.execute_reply.started":"2022-05-29T22:25:04.301159Z","shell.execute_reply":"2022-05-29T22:25:04.332626Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"birds_df.value_counts(\"data_set\").head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:04.334697Z","iopub.execute_input":"2022-05-29T22:25:04.335745Z","iopub.status.idle":"2022-05-29T22:25:04.349453Z","shell.execute_reply.started":"2022-05-29T22:25:04.335712Z","shell.execute_reply":"2022-05-29T22:25:04.348303Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# Frequency of bird species in the whole dataset\nprint(\"|species | f|\")\nbirds_df.value_counts(\"class_index\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:04.351758Z","iopub.execute_input":"2022-05-29T22:25:04.353200Z","iopub.status.idle":"2022-05-29T22:25:04.364049Z","shell.execute_reply.started":"2022-05-29T22:25:04.353164Z","shell.execute_reply":"2022-05-29T22:25:04.363095Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Look at csv entries for one single bird\n\n#mask = birds_df['labels'].str.contains(\"ABBOTTS BABBLER\") # Search for text fragment\n#mask = birds_df.query('labels == \"ABBOTTS BABBLER\"') # query for name (case sensitive!)\nmask = birds_df.loc[birds_df['class_index'] == 0]\nprint(mask.value_counts(\"data_set\"))\nmask","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:04.365214Z","iopub.execute_input":"2022-05-29T22:25:04.365986Z","iopub.status.idle":"2022-05-29T22:25:04.383683Z","shell.execute_reply.started":"2022-05-29T22:25:04.365928Z","shell.execute_reply":"2022-05-29T22:25:04.382896Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"## Image data","metadata":{}},{"cell_type":"code","source":"# File directories\nroot_dir = \"../input/100-bird-species\"\ntrain_dir = \"../input/100-bird-species/train\"\nvalid_dir = \"../input/100-bird-species/valid\"\ntest_dir = \"../input/100-bird-species/test\"","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:04.384730Z","iopub.execute_input":"2022-05-29T22:25:04.385567Z","iopub.status.idle":"2022-05-29T22:25:04.389753Z","shell.execute_reply.started":"2022-05-29T22:25:04.385531Z","shell.execute_reply":"2022-05-29T22:25:04.388991Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"### Plot a bird image","metadata":{}},{"cell_type":"code","source":"def showFirstBird(bird_name=\"MALLARD DUCK\"):\n    \"\"\"\n    Print out file paths of images in the valid_dir and show the first image of a given species.\n    \"\"\"\n    import glob\n    img_files = []\n    for img in glob.glob(os.path.join(valid_dir, bird_name)+\"/*\"):\n        img_files.append(img)\n        \n    for i in img_files:\n        print(i) # Print file path\n        ifile = tf.io.read_file(i) # Reads the contents of file\n        img_dec = tf.io.decode_image(ifile) # Decodes an image file\n        print(\"File shape: \", img_dec.shape, \"\\n\")\n        \n    img = PIL.Image.open(str(img_files[0]))\n    return img\n    \nshowFirstBird()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:04.391670Z","iopub.execute_input":"2022-05-29T22:25:04.392548Z","iopub.status.idle":"2022-05-29T22:25:04.437064Z","shell.execute_reply.started":"2022-05-29T22:25:04.392452Z","shell.execute_reply":"2022-05-29T22:25:04.436335Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"# 2. Create a dataset for the model\n## Generate tf.data.Dataset objects from a directory\nTake image files from a directory on disk and generate a ``tf.data.Dataset`` for train, validation and test dataset. ``image_dataset_from_directory()`` is a special TensorFlow data generator function.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nAchieving peak performance requires an efficient input pipeline that delivers data for \nthe next step before the current step has finished. The tf.data API helps to build flexible \nand efficient input pipelines.\n~ https://www.tensorflow.org/guide/data_performance\n\"\"\"\nIMAGE_SIZE=(150,150) # original size: 224,224 # Resolution decreased to speed up training time\nBATCH_SIZE=64 # default=32\nSEED=42\n#tf.random.set_seed(42)\n\ntrain_data = tf.keras.preprocessing.image_dataset_from_directory(\n    directory=train_dir,\n    label_mode='categorical',\n    batch_size=BATCH_SIZE, \n    image_size=IMAGE_SIZE,\n    seed=SEED,\n    shuffle=True   # default\n)\nclass_names = train_data.class_names\nnum_classes = len(class_names)\nprint(\"Class names: \", class_names[:5])\n\nvalid_data = tf.keras.preprocessing.image_dataset_from_directory(\n    directory=valid_dir,\n    label_mode='categorical',\n    batch_size=BATCH_SIZE,\n    image_size=IMAGE_SIZE,\n    seed=SEED,\n    shuffle=True   # default\n)\n\ntest_data = tf.keras.preprocessing.image_dataset_from_directory(\n    directory=test_dir,\n    label_mode='categorical',\n    batch_size=BATCH_SIZE,\n    image_size=IMAGE_SIZE,\n    seed=SEED,\n    shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:04.438140Z","iopub.execute_input":"2022-05-29T22:25:04.438926Z","iopub.status.idle":"2022-05-29T22:25:08.733629Z","shell.execute_reply.started":"2022-05-29T22:25:04.438893Z","shell.execute_reply":"2022-05-29T22:25:08.732850Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:08.735611Z","iopub.execute_input":"2022-05-29T22:25:08.735992Z","iopub.status.idle":"2022-05-29T22:25:08.743576Z","shell.execute_reply.started":"2022-05-29T22:25:08.735939Z","shell.execute_reply":"2022-05-29T22:25:08.742697Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"## Configure the dataset for performance\nTo prevent I/O blocking while retrieving data from disk we use buffered prefetching.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nThe tf.data API provides the tf.data.Dataset.prefetch transformation. It can be used \nto decouple the time when data is produced from the time when data is consumed. In particular, \nthe transformation uses a background thread and an internal buffer to prefetch elements from \nthe input dataset ahead of the time they are requested. The number of elements to prefetch \nshould be equal to (or possibly greater than) the number of batches consumed by a single training step. \nYou could either manually tune this value, or set it to tf.data.AUTOTUNE, which will prompt the \ntf.data runtime to tune the value dynamically at runtime.\n\n ~ https://www.tensorflow.org/guide/data_performance\n\"\"\"\n# without cache()\ntrain_data_pf = train_data.prefetch(buffer_size = tf.data.AUTOTUNE)\nvalid_data_pf = valid_data.prefetch(buffer_size = tf.data.AUTOTUNE)\ntest_data_pf = test_data.prefetch(buffer_size = tf.data.AUTOTUNE)\n\n# with cache()\n\"\"\"\nDataset.cache keeps the images in memory after they're loaded off disk during the first epoch. \nThis will ensure the dataset does not become a bottleneck while training your model. If your dataset is \ntoo large to fit into memory, you can also use this method to create a performant on-disk cache.\n\n ~ https://www.tensorflow.org/tutorials/images/classification#configure_the_dataset_for_performance\n\"\"\"\ntrain_data_pfc = train_data.cache().prefetch(buffer_size = tf.data.AUTOTUNE)\nvalid_data_pfc = valid_data.cache().prefetch(buffer_size = tf.data.AUTOTUNE)\ntest_data_pfc = test_data.cache().prefetch(buffer_size = tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:08.744899Z","iopub.execute_input":"2022-05-29T22:25:08.745383Z","iopub.status.idle":"2022-05-29T22:25:08.758017Z","shell.execute_reply.started":"2022-05-29T22:25:08.745344Z","shell.execute_reply":"2022-05-29T22:25:08.757225Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the generated data\nAlso important to check reproducable split.","metadata":{}},{"cell_type":"markdown","source":"# 2.1 Standardize the data (Preprocessing)\nRGB channel values range between [0, 255]. This is not ideal for a neural network; in general you should seek to make your input values small.\n\nThis can be achieved through:\n- data standardization \n- image segmentation ? \n- PCA ? ","metadata":{}},{"cell_type":"code","source":"# This layer will standardize values to be in the range [0, 1]\nnormalization_layer = layers.Rescaling(1./IMAGE_SIZE[0])\n\n# In order to use this layer we will call it at the start of our model definition.","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:08.759259Z","iopub.execute_input":"2022-05-29T22:25:08.760060Z","iopub.status.idle":"2022-05-29T22:25:08.766823Z","shell.execute_reply.started":"2022-05-29T22:25:08.760021Z","shell.execute_reply":"2022-05-29T22:25:08.766001Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"# 3. Build the Model ","metadata":{}},{"cell_type":"markdown","source":"## Define layers\n**Take aways from [Gabriel Atkin's Age Prediction From Facial Images](https://www.youtube.com/watch?v=9AnCNBL8c6Q&t=661s):**\n- Recurrent feature extraction\n- Flatten layer\n    - layers.Flatten()(x) # sometimes too many features\n    - layers.GlobalAveragePooling2D()(x) # average across the first 2 dimensions\n- ","metadata":{}},{"cell_type":"code","source":"# Input layer\ninputs = tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n\n# First layer\nx = layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu')(inputs) # convolution\nx = layers.MaxPool2D()(x) # downsampling\n\n# Second layer\nx = layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu')(x) # convolution\nx = layers.MaxPool2D()(x) # downsampling\n\n# Third layer\nx = layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu')(x) # convolution\nx = layers.MaxPool2D()(x) # downsampling\n\n# Flattening layer\nx = layers.GlobalAveragePooling2D()(x) \n\n# Dense layer\nx = layers.Dense(units=128, activation='relu')(x) # twice as many units as features after flattening\nx = layers.Dense(units=128, activation='relu')(x) \nx = layers.Dense(units=128, activation='relu')(x) #new test model04\nx = layers.Dense(units=128, activation='relu')(x) #new test model04\n\n# Output layer\noutputs = layers.Dense(units=num_classes, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:08.768081Z","iopub.execute_input":"2022-05-29T22:25:08.768472Z","iopub.status.idle":"2022-05-29T22:25:08.864789Z","shell.execute_reply.started":"2022-05-29T22:25:08.768436Z","shell.execute_reply":"2022-05-29T22:25:08.864028Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"inputs\n#<KerasTensor: shape=(None, 150, 150, 3) dtype=float32 (created by layer 'input_1')>","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:08.866138Z","iopub.execute_input":"2022-05-29T22:25:08.866659Z","iopub.status.idle":"2022-05-29T22:25:08.872690Z","shell.execute_reply.started":"2022-05-29T22:25:08.866623Z","shell.execute_reply":"2022-05-29T22:25:08.871462Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"x\n# ... after the first convolution\n#<KerasTensor: shape=(None, 148, 148, 16) dtype=float32 (created by layer 'conv2d_6')>\n# ... after downsampling\n#<KerasTensor: shape=(None, 74, 74, 16) dtype=float32 (created by layer 'max_pooling2d_6')>\n##############################################################################################\n# What happened up there?\n# We lost 2 pixels (150->148) and we now have 16 2D features.","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:08.882476Z","iopub.execute_input":"2022-05-29T22:25:08.882769Z","iopub.status.idle":"2022-05-29T22:25:08.892802Z","shell.execute_reply.started":"2022-05-29T22:25:08.882745Z","shell.execute_reply":"2022-05-29T22:25:08.891865Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"outputs","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:08.894541Z","iopub.execute_input":"2022-05-29T22:25:08.895249Z","iopub.status.idle":"2022-05-29T22:25:08.904552Z","shell.execute_reply.started":"2022-05-29T22:25:08.895207Z","shell.execute_reply":"2022-05-29T22:25:08.903103Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"## Compile model","metadata":{}},{"cell_type":"code","source":"#optimizer = tf.keras.optimizers.RMSprop()\n#optimizer = tf.keras.optimizers.Adam()\n#optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n# ~ https://blog.seeso.io/a-simple-guide-to-speed-up-your-training-in-tensorflow-2-8386e6411be4\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='mse',          # MSE for regression task\n    metrics=['accuracy']\n)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:08.907220Z","iopub.execute_input":"2022-05-29T22:25:08.908274Z","iopub.status.idle":"2022-05-29T22:25:08.925883Z","shell.execute_reply.started":"2022-05-29T22:25:08.908234Z","shell.execute_reply":"2022-05-29T22:25:08.924776Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"### Plot the model architecture","metadata":{}},{"cell_type":"code","source":"dot_img_file = '/tmp/'+model.name+'.png'\nmodel_img = tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\nmodel_img","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:39:00.479457Z","iopub.execute_input":"2022-05-29T22:39:00.479850Z","iopub.status.idle":"2022-05-29T22:39:00.695739Z","shell.execute_reply.started":"2022-05-29T22:39:00.479814Z","shell.execute_reply":"2022-05-29T22:39:00.694842Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"## Train model\n**Speed training time**","metadata":{}},{"cell_type":"code","source":"# Switch on XLA\n#tf.config.optimizer.set_jit(True) # -> WARNING","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:09.799246Z","iopub.execute_input":"2022-05-29T22:25:09.809064Z","iopub.status.idle":"2022-05-29T22:25:09.816177Z","shell.execute_reply.started":"2022-05-29T22:25:09.809015Z","shell.execute_reply":"2022-05-29T22:25:09.815167Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 5\nSTEPS = int(len(train_data_pf)*0.01)\nVALIDATION_STEPS = int(len(valid_data_pf)*0.1)\n\nprint(\"|Epochs:\", EPOCHS, \"| Steps:\", STEPS, \"| Validation steps:\", VALIDATION_STEPS, \n      \"|\\n**********************************************\")\n\n\nhistory = model.fit(\n    train_data_pf,\n    validation_data=valid_data_pf,\n    validation_steps=10, # at the end of each epoch\n    epochs=EPOCHS,\n    workers=-1,\n    verbose=1,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping( \n            #Prevent overfitting through early stopping\n            monitor=\"val_loss\",\n            patience=5,\n            restore_best_weights=True,\n            verbose=1\n)])\n\nmodel.save(\"birds.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:25:09.818045Z","iopub.execute_input":"2022-05-29T22:25:09.818436Z","iopub.status.idle":"2022-05-29T22:30:46.688421Z","shell.execute_reply.started":"2022-05-29T22:25:09.818400Z","shell.execute_reply":"2022-05-29T22:30:46.687620Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"### Questions:\n- **What is ``tf.keras.layers.Conv2D`` ?**\n    - Specifies a convolution layer.\n    - Let's look at an **example** ([Details](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D#args_1)): <br>\nlayers.Conv2D(  \n...filters=16,   # dimensionality of the output space  \n...kernel_size=3,   # int or tuple, height and width of the 2D convolution window  \n...strides=(1,1),   # DEFAULT  \n...padding='same',   # 'valid' -> no padding, 'same' -> same as input  \n...groups=1,   # DEFAULT; applies filter to smaller parts of the image (~ Lupe)  \n...activation='relu'   # there are many more [here](https://www.tensorflow.org/api_docs/python/tf/keras/activations)  \n) \n\n\n- **What is ``tf.keras.layers.MaxPooling2D`` ?**\n    - Downsamples the input along its spatial dimensions by taking the maximum value over an input window (size=``pool_size``) for each channel of the input\n    - ``strides`` will shift the window\n    - ``padding`` changes output size only if 'valid'  \n    \n    \n- **What is ``tf.keras.layers.Flatten`` ?**\n    - Flattens the input. Does not affect the batch size.\n    \n    \n- **What is ``tf.keras.layers.Dropout`` ?**\n    - randomly sets input units to 0 with a frequency of ``rate`` at each step during training\n    - helps to prevent overfitting\n    \n    \n- **What is ``tf.keras.layers.Dense`` ?**\n    - ``units`` is a positive integer, dimensionality of the output space.\n    - specify activation function to use with ``activation`` (DEFAULT='linear')\n    \n\nSource: http://bit.ly/2lXXdw5 (the Rock-Paper-Scissor model from YT)","metadata":{}},{"cell_type":"markdown","source":"### Model architecture from TensorFlow tutorial ([Link](https://www.tensorflow.org/tutorials/images/classification#:~:text=layers.Resizing%20layer.-,Create%20the%20model,-The%20Sequential%20model))\nThe Sequential model consists of three convolution blocks (``tf.keras.layers.Conv2D``) with a max pooling layer (``tf.keras.layers.MaxPooling2D``) in each of them. <br>\nThere's a fully-connected layer (``tf.keras.layers.Dense``) with 128 units on top of it that is activated by a ReLU activation function ('**relu**'). <br>\nThis model has not been tuned for high accuracy.","metadata":{}},{"cell_type":"code","source":"# My model adapted from web\n\n\"\"\"INPUT_SHAPE=(150, 150, 3)\n\n# 16, 32, 64, 128, 512 -> BUT number should decrease towards the number of classes (400)\nneurons = [16, 32, 64, 128]\n\nmodel = tf.keras.models.Sequential([\n    # Rescaling layer to standardize data (Preprocessing)\n    #layers.Rescaling(scale=1./INPUT_SHAPE[0], input_shape=INPUT_SHAPE),\n    \n    # The first convolution layer\n    layers.Conv2D(\n        input_shape=INPUT_SHAPE,\n        filters=neurons[0], # 16, 32, 64\n        kernel_size=(3,3), \n        padding='same', \n        activation='relu', \n        strides=(1,1),\n        groups=1\n    ),\n    layers.MaxPooling2D(\n        pool_size=(2,2), # DEFAULT\n        strides=None, # DEFAULT\n        padding='same', # DEFAULT\n    ),\n    \n    # The second convolution layer\n    layers.Conv2D(\n        filters=neurons[1], # 16, 32, 64\n        kernel_size=(3,3), \n        padding='same', \n        activation='relu', \n        strides=(1,1),\n        groups=1\n    ),\n    layers.MaxPooling2D(),\n    \n    # The third convolution layer\n    layers.Conv2D(\n        filters=neurons[2], # 16, 32, 64\n        kernel_size=(3,3), \n        padding='same', \n        activation='relu', \n        strides=(1,1),\n        groups=1\n    ),\n    layers.MaxPooling2D(),\n    \n    # Process data for dense layers\n    layers.Flatten(),\n    #layers.Dropout(0.2), # higher -> more regularization \n    layers.BatchNormalization(),\n\n\n    # First fully-connected layer\n    layers.Dense(\n        units=neurons[3], # 128\n        activation='relu'\n    ),\n    \n    # Last fully-connected layer\n    layers.Dense(\n        units=num_classes,\n        activation='softmax',\n    )\n])\"\"\";","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:30:46.689986Z","iopub.execute_input":"2022-05-29T22:30:46.690345Z","iopub.status.idle":"2022-05-29T22:30:46.697925Z","shell.execute_reply.started":"2022-05-29T22:30:46.690309Z","shell.execute_reply":"2022-05-29T22:30:46.697128Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Basically model from web\n\n\"\"\"INPUT_SHAPE=(224, 224,3) \n\nmodel = tf.keras.models.Sequential([\n    # Rescaling layer to standardize data (Preprocessing)\n    #layers.Rescaling(scale=1./INPUT_SHAPE[0], input_shape=INPUT_SHAPE),\n    \n    # The first convolution layer\n    layers.Conv2D(\n        input_shape=INPUT_SHAPE,\n        filters=16, # 16, 32, 64\n        kernel_size=(3,3), \n        padding='valid', \n        activation='relu', \n        strides=(1,1),\n        groups=1\n    ),\n    layers.MaxPooling2D(\n        pool_size=(2,2), # DEFAULT\n        strides=None, # DEFAULT\n        padding='valid', # DEFAULT\n    ),\n    \n    # The second convolution layer\n    layers.Conv2D(\n        filters=32, # 16, 32, 64\n        kernel_size=(3,3), \n        padding='valid', \n        activation='relu', \n        strides=(1,1),\n        groups=1\n    ),\n    layers.MaxPooling2D(),\n    \n    # The third convolution layer\n    layers.Conv2D(\n        filters=64, # 16, 32, 64\n        kernel_size=(3,3), \n        padding='valid', \n        activation='relu', \n        strides=(1,1),\n        groups=1\n    ),\n    layers.MaxPooling2D(),\n    \n    # Process data for dense layers\n    layers.Flatten(),\n    #tf.keras.layers.Dropout(0.2), # higher -> more regularization \n\n    # First fully-connected layer\n    layers.Dense(\n        units=128, \n        activation='relu'\n    ),\n    \n    # Last fully-connected layer\n    layers.Dense(\n        units=num_classes,\n        activation=None,\n    )\n])\"\"\";","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:30:46.699200Z","iopub.execute_input":"2022-05-29T22:30:46.699621Z","iopub.status.idle":"2022-05-29T22:30:46.712597Z","shell.execute_reply.started":"2022-05-29T22:30:46.699583Z","shell.execute_reply":"2022-05-29T22:30:46.711800Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"**Optimizers:**\n- Adam: a sensible default optimizer\n- Nadam (Nesterov-accelerated Adam)\n- RMSProp (oftentimes used for regression) -> keras default","metadata":{}},{"cell_type":"code","source":"\"\"\"model.compile(\n    optimizer='adam', # DEFAULT 'rmsprop'\n    loss='mean_absolute_error', \n    metrics=['accuracy']\n)\"\"\"\n\n# I used this last time\n\"\"\"model.compile(\n    loss=keras.losses.categorical_crossentropy,\n    optimizer='adam',\n    metrics=['accuracy']\n)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:30:46.714087Z","iopub.execute_input":"2022-05-29T22:30:46.714459Z","iopub.status.idle":"2022-05-29T22:30:46.726766Z","shell.execute_reply.started":"2022-05-29T22:30:46.714422Z","shell.execute_reply":"2022-05-29T22:30:46.726038Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"#model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:30:46.727791Z","iopub.execute_input":"2022-05-29T22:30:46.728133Z","iopub.status.idle":"2022-05-29T22:30:46.733473Z","shell.execute_reply.started":"2022-05-29T22:30:46.728076Z","shell.execute_reply":"2022-05-29T22:30:46.732593Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"### Links:\n- TF model.fit() -> https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n- ...","metadata":{}},{"cell_type":"markdown","source":"# 4. Train model","metadata":{}},{"cell_type":"code","source":"\"\"\"EPOCHS = 5\nSTEPS = int(len(train_data_pf)*0.01)\nVALIDATION_STEPS = int(len(valid_data_pf)*0.1)\nprint(\"|Epochs:\", EPOCHS, \"| Steps:\", STEPS, \"| Validation steps:\", VALIDATION_STEPS, \n      \"|\\n**********************************************\")\n\nhistory = model.fit(   .\n    train_data_pf, \n    epochs=EPOCHS, \n    steps_per_epoch=STEPS, \n    validation_data=valid_data_pf, \n    validation_steps=VALIDATION_STEPS, #3\n    verbose=1\n)\n\nmodel.save(\"birds.h5\")\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:30:46.735122Z","iopub.execute_input":"2022-05-29T22:30:46.735647Z","iopub.status.idle":"2022-05-29T22:30:46.743014Z","shell.execute_reply.started":"2022-05-29T22:30:46.735608Z","shell.execute_reply":"2022-05-29T22:30:46.742206Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"## Training Results\n### Visualize Training","metadata":{}},{"cell_type":"code","source":"def plotHistory(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(len(acc))\n\n    plt.plot(epochs, acc, 'r', label='Training accuracy')\n    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n    #plt.plot(epochs, loss, 'g', label='Training loss')\n    #plt.plot(epochs, val_loss, 'o', label='Validation loss')\n    \n    plt.title('Training and validation accuracy and loss')\n    plt.legend(loc=0)\n    plt.figure()\n\n    plt.show()\n\nplotHistory(history)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:30:46.744235Z","iopub.execute_input":"2022-05-29T22:30:46.745013Z","iopub.status.idle":"2022-05-29T22:30:46.965782Z","shell.execute_reply.started":"2022-05-29T22:30:46.744970Z","shell.execute_reply":"2022-05-29T22:30:46.965026Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"def plotHistory2(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs_range = range(EPOCHS)\n\n    plt.figure(figsize=(16, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend()#loc='lower right')\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend()#loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.show()\n    \nplotHistory2(history)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:30:46.967238Z","iopub.execute_input":"2022-05-29T22:30:46.967814Z","iopub.status.idle":"2022-05-29T22:30:47.298631Z","shell.execute_reply.started":"2022-05-29T22:30:46.967777Z","shell.execute_reply":"2022-05-29T22:30:47.297857Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"# 5. Evaluation","metadata":{}},{"cell_type":"code","source":"scores = model.evaluate(test_data_pf, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","metadata":{"execution":{"iopub.status.busy":"2022-05-29T22:30:47.299744Z","iopub.execute_input":"2022-05-29T22:30:47.300126Z","iopub.status.idle":"2022-05-29T22:30:49.879319Z","shell.execute_reply.started":"2022-05-29T22:30:47.300091Z","shell.execute_reply":"2022-05-29T22:30:49.877517Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"---\n# README\nafter the first meeting\n\n### Aufteilung\n- keras tensorflow - Clemens\n- pytorch - Jakob\n- PCA + preprocessing - Lena \n\n\n### Methoden \n- pca?\n- image segementation\n- Wie laden wir die Bilder von der CSV ins Notebook?\n- Wieviele Datenreihen brauchen wir? \ndata set\n| train    58388 | test      2000 | valid     2000 |\n","metadata":{}},{"cell_type":"markdown","source":"# Links\n## Good Notebook for reference\n### https://www.kaggle.com/code/ashwinshetgaonkar/bird-classifier-tensorflow-beginner\n\n## Learn TensorFlow in this notebook\n### http://bit.ly/2lXXdw5","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}