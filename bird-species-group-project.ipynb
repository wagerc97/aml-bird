{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bird Species Classifier for AML project using Keras/TensorFlow\n## University of Vienna, SS 2022","metadata":{"execution":{"iopub.status.busy":"2022-05-27T16:35:47.603306Z","iopub.execute_input":"2022-05-27T16:35:47.603691Z","iopub.status.idle":"2022-05-27T16:35:47.607886Z","shell.execute_reply.started":"2022-05-27T16:35:47.603659Z","shell.execute_reply":"2022-05-27T16:35:47.607014Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# To search directories\nimport os\nimport glob\n\n# To visualize data\nimport PIL\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#sns.set_style('darkgrid')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:01.88039Z","iopub.execute_input":"2022-05-31T06:43:01.8808Z","iopub.status.idle":"2022-05-31T06:43:01.886653Z","shell.execute_reply.started":"2022-05-31T06:43:01.88077Z","shell.execute_reply":"2022-05-31T06:43:01.885456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"TensorFlow Version: \", tf.__version__)\nprint(\"Keras Version: \", keras.__version__)\nprint(\"GPU devices: \", tf.config.list_physical_devices('gpu'))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:01.898188Z","iopub.execute_input":"2022-05-31T06:43:01.898539Z","iopub.status.idle":"2022-05-31T06:43:01.904868Z","shell.execute_reply.started":"2022-05-31T06:43:01.898509Z","shell.execute_reply":"2022-05-31T06:43:01.903078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Examine and understand data\n## CSV data\nThe ``birds.csv`` contains information of the dataset. Let's look into the structure of the data.","metadata":{}},{"cell_type":"code","source":"# Create a dataframe from the csv\nbirds_df = pd.read_csv(\"../input/100-bird-species/birds.csv\")\n# clean column names\nbirds_df.columns = [col.replace(' ', '_').lower() for col in birds_df.columns]\nbirds_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:01.914872Z","iopub.execute_input":"2022-05-31T06:43:01.915145Z","iopub.status.idle":"2022-05-31T06:43:02.005472Z","shell.execute_reply.started":"2022-05-31T06:43:01.915119Z","shell.execute_reply":"2022-05-31T06:43:02.004472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"birds_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:02.007236Z","iopub.execute_input":"2022-05-31T06:43:02.007672Z","iopub.status.idle":"2022-05-31T06:43:02.039924Z","shell.execute_reply.started":"2022-05-31T06:43:02.00763Z","shell.execute_reply":"2022-05-31T06:43:02.039072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"birds_df.value_counts(\"data_set\").head()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:02.041286Z","iopub.execute_input":"2022-05-31T06:43:02.041854Z","iopub.status.idle":"2022-05-31T06:43:02.057137Z","shell.execute_reply.started":"2022-05-31T06:43:02.041813Z","shell.execute_reply":"2022-05-31T06:43:02.056419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Frequency of bird species in the whole dataset\nprint(\"|species | f|\")\nbirds_df.value_counts(\"class_index\")","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:02.059213Z","iopub.execute_input":"2022-05-31T06:43:02.059948Z","iopub.status.idle":"2022-05-31T06:43:02.071457Z","shell.execute_reply.started":"2022-05-31T06:43:02.059909Z","shell.execute_reply":"2022-05-31T06:43:02.070631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at csv entries for one single bird\n\n#mask = birds_df['labels'].str.contains(\"ABBOTTS BABBLER\") # Search for text fragment\n#mask = birds_df.query('labels == \"ABBOTTS BABBLER\"') # query for name (case sensitive!)\nmask = birds_df.loc[birds_df['class_index'] == 0]\nprint(mask.value_counts(\"data_set\"))\nmask","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:02.07262Z","iopub.execute_input":"2022-05-31T06:43:02.073605Z","iopub.status.idle":"2022-05-31T06:43:02.092221Z","shell.execute_reply.started":"2022-05-31T06:43:02.073553Z","shell.execute_reply":"2022-05-31T06:43:02.091458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image data","metadata":{}},{"cell_type":"code","source":"# File directories\nroot_dir = \"../input/100-bird-species\"\ntrain_dir = \"../input/100-bird-species/train\"\nvalid_dir = \"../input/100-bird-species/valid\"\ntest_dir = \"../input/100-bird-species/test\"","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:02.093468Z","iopub.execute_input":"2022-05-31T06:43:02.093969Z","iopub.status.idle":"2022-05-31T06:43:02.098414Z","shell.execute_reply.started":"2022-05-31T06:43:02.093933Z","shell.execute_reply":"2022-05-31T06:43:02.0975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot a bird image","metadata":{}},{"cell_type":"code","source":"def showFirstBird(bird_name=\"MALLARD DUCK\"):\n    \"\"\"\n    Print out file paths of images in the valid_dir and show the first image of a given species.\n    \"\"\"\n    import glob\n    img_files = []\n    for img in glob.glob(os.path.join(valid_dir, bird_name)+\"/*\"):\n        img_files.append(img)\n        \n    for i in img_files:\n        print(i) # Print file path\n        ifile = tf.io.read_file(i) # Reads the contents of file\n        img_dec = tf.io.decode_image(ifile) # Decodes an image file\n        print(\"File shape: \", img_dec.shape, \"\\n\")\n        \n    img = PIL.Image.open(str(img_files[0]))\n    return img\n    \nshowFirstBird()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:02.099937Z","iopub.execute_input":"2022-05-31T06:43:02.100782Z","iopub.status.idle":"2022-05-31T06:43:02.147841Z","shell.execute_reply.started":"2022-05-31T06:43:02.100744Z","shell.execute_reply":"2022-05-31T06:43:02.146979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Create a dataset for the model\n## Generate tf.data.Dataset objects from a directory\nTake image files from a directory on disk and generate a ``tf.data.Dataset`` for train, validation and test dataset. ``image_dataset_from_directory()`` is a special TensorFlow data generator function.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nAchieving peak performance requires an efficient input pipeline that delivers data for \nthe next step before the current step has finished. The tf.data API helps to build flexible \nand efficient input pipelines.\n~ https://www.tensorflow.org/guide/data_performance\n\"\"\"\nIMAGE_SIZE=(150,150) # original size: 224,224 # Resolution decreased to speed up training time\nBATCH_SIZE=32 # default=32\nSEED=42\nnp.random.seed(42)\ntf.random.set_seed(42)\n\ntrain_data = image_dataset_from_directory(\n    directory=train_dir,\n    validation_split=0.5,\n    label_mode='categorical',\n    batch_size=BATCH_SIZE, \n    image_size=IMAGE_SIZE,\n    subset='training',\n    seed=SEED,\n    shuffle=True   # default\n)\nclass_names = train_data.class_names\nnum_classes = len(class_names)\n#print(\"Class names: \", class_names[:5])\n\nvalid_data = image_dataset_from_directory(\n    directory=valid_dir,\n    label_mode='categorical',\n    batch_size=BATCH_SIZE,\n    image_size=IMAGE_SIZE,\n    seed=SEED,\n    shuffle=True   # default\n)\n\ntest_data = image_dataset_from_directory(\n    directory=test_dir,\n    label_mode='categorical',\n    batch_size=BATCH_SIZE,\n    image_size=IMAGE_SIZE,\n    seed=SEED,\n    shuffle=False\n)\nlen(train_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:02.149203Z","iopub.execute_input":"2022-05-31T06:43:02.149747Z","iopub.status.idle":"2022-05-31T06:43:06.549603Z","shell.execute_reply.started":"2022-05-31T06:43:02.14971Z","shell.execute_reply":"2022-05-31T06:43:06.548756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configure the dataset for performance\nTo prevent I/O blocking while retrieving data from disk we use buffered prefetching.  \n\nThe tf.data API provides the tf.data.Dataset.prefetch transformation. It can be used \nto decouple the time when data is produced from the time when data is consumed. In particular, \nthe transformation uses a background thread and an internal buffer to prefetch elements from \nthe input dataset ahead of the time they are requested. The number of elements to prefetch \nshould be equal to (or possibly greater than) the number of batches consumed by a single training step. \nYou could either manually tune this value, or set it to tf.data.AUTOTUNE, which will prompt the \ntf.data runtime to tune the value dynamically at runtime.\n\n ~ https://www.tensorflow.org/guide/data_performance","metadata":{}},{"cell_type":"code","source":"train_data_pf = train_data.prefetch(buffer_size = tf.data.AUTOTUNE)\nvalid_data_pf = valid_data.prefetch(buffer_size = tf.data.AUTOTUNE)\ntest_data_pf = test_data.prefetch(buffer_size = tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:06.551254Z","iopub.execute_input":"2022-05-31T06:43:06.551893Z","iopub.status.idle":"2022-05-31T06:43:06.558409Z","shell.execute_reply.started":"2022-05-31T06:43:06.551855Z","shell.execute_reply":"2022-05-31T06:43:06.557575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Create Model ","metadata":{}},{"cell_type":"markdown","source":"## Define model architecture","metadata":{}},{"cell_type":"code","source":"INPUT_SHAPE=(150, 150, 3)\n\nmodel = tf.keras.models.Sequential([\n    \n    # handy input layer\n    layers.Input(INPUT_SHAPE),\n    \n    # convolution layers\n    layers.Conv2D(128, (3,3), activation='relu'),\n    layers.MaxPooling2D(pool_size=2),\n    layers.BatchNormalization(),    \n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.MaxPooling2D(pool_size=2),\n    layers.BatchNormalization(),\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D(pool_size=2),\n    layers.BatchNormalization(),\n    layers.Conv2D(16, (3,3), activation='relu'),\n    layers.MaxPooling2D(pool_size=2),\n    layers.BatchNormalization(),\n    layers.Conv2D(8, (3,3), activation='relu'),\n    layers.MaxPooling2D(pool_size=2),\n    layers.BatchNormalization(),\n    \n    # Last fully-connected layer\n    layers.Flatten(input_shape=INPUT_SHAPE),\n    #layers.Dropout(0.2), # higher -> more regularization \n    layers.BatchNormalization(),\n    layers.Dense(units=num_classes, activation='softmax')\n])\nkeras.backend.clear_session()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:06.562218Z","iopub.execute_input":"2022-05-31T06:43:06.562623Z","iopub.status.idle":"2022-05-31T06:43:06.733223Z","shell.execute_reply.started":"2022-05-31T06:43:06.56259Z","shell.execute_reply":"2022-05-31T06:43:06.732422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compile model\n- Cross-entropy is the default loss function to use for multi-class classification problems. ~ [Link](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/#:~:text=Cross%2Dentropy%20is%20the%20default%20loss%20function%20to%20use%20for%20multi%2Dclass%20classification%20problems.)\n    - IMPORTANT: The function requires that the output layer is configured with an n nodes (one for each class), in this case three nodes, and a ‘softmax‘ activation in order to predict the probability for each class.","metadata":{}},{"cell_type":"code","source":"# Configure optimizer\nopt = \n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy', # standard for multi-class clf    \n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:06.735232Z","iopub.execute_input":"2022-05-31T06:43:06.73582Z","iopub.status.idle":"2022-05-31T06:43:06.748357Z","shell.execute_reply.started":"2022-05-31T06:43:06.735783Z","shell.execute_reply":"2022-05-31T06:43:06.747453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot the model architecture","metadata":{}},{"cell_type":"code","source":"dot_img_file = '/tmp/'+model.name+'.png'\nmodel_img = tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n#model_img","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:06.749824Z","iopub.execute_input":"2022-05-31T06:43:06.750298Z","iopub.status.idle":"2022-05-31T06:43:07.907591Z","shell.execute_reply.started":"2022-05-31T06:43:06.750258Z","shell.execute_reply":"2022-05-31T06:43:07.906142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"code","source":"EPOCHS = 15\nSTEPS = int(len(train_data_pf))\nVALIDATION_STEPS = int(len(valid_data_pf)*0.1)\n\nheader=f\"|Epochs: {EPOCHS} | Steps: {STEPS} | Validation steps: {VALIDATION_STEPS}\"\nstars = \"\\n\"+(\"*\"*len(header)); print(header, stars)\n\nhistory = model.fit(\n    train_data_pf,\n    validation_data=valid_data_pf,\n    validation_steps=10, # at the end of each epoch\n    epochs=EPOCHS,\n    workers=-1, verbose=1, callbacks=[\n        tf.keras.callbacks.EarlyStopping( \n            #Prevent overfitting through early stopping\n            monitor=\"val_loss\",\n            patience=5,\n            restore_best_weights=True,\n            verbose=1\n)])\nmodel.save(\"birds.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:43:07.911635Z","iopub.execute_input":"2022-05-31T06:43:07.91197Z","iopub.status.idle":"2022-05-31T06:54:37.847697Z","shell.execute_reply.started":"2022-05-31T06:43:07.911936Z","shell.execute_reply":"2022-05-31T06:54:37.846869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation\nscores = model.evaluate(test_data_pf, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:54:37.849021Z","iopub.execute_input":"2022-05-31T06:54:37.849367Z","iopub.status.idle":"2022-05-31T06:54:44.275717Z","shell.execute_reply.started":"2022-05-31T06:54:37.849332Z","shell.execute_reply":"2022-05-31T06:54:44.274872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Train model","metadata":{}},{"cell_type":"markdown","source":"## Training Results\n### Visualize Training","metadata":{}},{"cell_type":"code","source":"def plotHistory(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(len(acc))\n\n    plt.plot(epochs, acc, 'r', label='Training accuracy')\n    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n    #plt.plot(epochs, loss, 'g', label='Training loss')\n    #plt.plot(epochs, val_loss, 'o', label='Validation loss')\n    \n    plt.title('Training and validation accuracy and loss')\n    plt.legend(loc=0)\n    plt.figure()\n\n    plt.show()\n\nplotHistory(history)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:54:44.349717Z","iopub.execute_input":"2022-05-31T06:54:44.350372Z","iopub.status.idle":"2022-05-31T06:54:44.56224Z","shell.execute_reply.started":"2022-05-31T06:54:44.350331Z","shell.execute_reply":"2022-05-31T06:54:44.561381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotHistory2(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs_range = range(EPOCHS)\n\n    plt.figure(figsize=(16, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend()#loc='lower right')\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend()#loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.show()\n    \nplotHistory2(history)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:54:44.563428Z","iopub.execute_input":"2022-05-31T06:54:44.564637Z","iopub.status.idle":"2022-05-31T06:54:44.880982Z","shell.execute_reply.started":"2022-05-31T06:54:44.564597Z","shell.execute_reply":"2022-05-31T06:54:44.880175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Evaluation","metadata":{}},{"cell_type":"code","source":"scores = model.evaluate(test_data_pf, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T06:54:44.882325Z","iopub.execute_input":"2022-05-31T06:54:44.882674Z","iopub.status.idle":"2022-05-31T06:54:46.854304Z","shell.execute_reply.started":"2022-05-31T06:54:44.882638Z","shell.execute_reply":"2022-05-31T06:54:46.853476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# README\nafter the first meeting\n\n### Aufteilung\n- keras tensorflow - Clemens\n- pytorch - Jakob\n- PCA + preprocessing - Lena \n\n\n### Methoden \n- pca?\n- image segementation\n- Wie laden wir die Bilder von der CSV ins Notebook?\n- Wieviele Datenreihen brauchen wir? \ndata set\n| train    58388 | test      2000 | valid     2000 |\n","metadata":{}},{"cell_type":"markdown","source":"**Take aways from [Gabriel Atkin's Age Prediction From Facial Images](https://www.youtube.com/watch?v=9AnCNBL8c6Q&t=661s):**\n- Recurrent feature extraction\n- Flatten layer\n    - layers.Flatten()(x) # sometimes too many features\n    - layers.GlobalAveragePooling2D()(x) # average across the first 2 dimensions\n\n\"\"\"    # convolution layer\n    layers.Conv2D(16, (3,3), activation='relu', input_shape=INPUT_SHAPE),\n    layers.MaxPooling2D(),\n    \n    # convolution layer\n    layers.Conv2D(32, (3,3), activation='relu'),\n    layers.MaxPooling2D(),\n    \n    # convolution layer\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.MaxPooling2D(),\n    \n    # convolution layer\n    layers.Conv2D(128, (3,3), activation='relu'),\n    layers.MaxPooling2D(),\"\"\"","metadata":{}},{"cell_type":"markdown","source":"### Links:\n- TF model.fit() -> https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n- ...","metadata":{}},{"cell_type":"markdown","source":"**Optimizers:**\n- Adam: a sensible default optimizer\n- Nadam (Nesterov-accelerated Adam)\n- RMSProp (oftentimes used for regression) -> keras default","metadata":{}},{"cell_type":"markdown","source":"# Links\n## Good Notebook for reference\n### https://www.kaggle.com/code/ashwinshetgaonkar/bird-classifier-tensorflow-beginner\n\n## Learn TensorFlow in this notebook\n### http://bit.ly/2lXXdw5","metadata":{}}]}